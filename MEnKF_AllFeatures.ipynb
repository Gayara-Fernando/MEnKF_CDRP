{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ac014d-d291-422e-aea0-705eb7b4c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import random\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import multivariate_normal as mvn\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import block_diag\n",
    "import warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7646af0f-ee7d-4761-b6b5-06bf7b4e183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9703c7-dc82-42bb-b7e1-6ebc3393dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the extracted embeddings first \n",
    "# cdr embeddings - valid\n",
    "cdr_test_mut = np.load(\"saved_output_data//Valid_mut_new_split_CDR.npy\")\n",
    "cdr_test_methyl = np.load(\"saved_output_data//Valid_methyl_new_split_CDR.npy\")\n",
    "cdr_test_gene = np.load(\"saved_output_data//Valid_gene_new_split_CDR.npy\")\n",
    "cdr_test_drugs = np.load(\"saved_output_data//Valid_drug_new_split_CDR.npy\")\n",
    "test_y = np.load(\"saved_output_data//Valid_y.npy\")\n",
    "\n",
    "# gcn embeddings - valid\n",
    "gcn_test_cnv = np.load(\"saved_output_data//Valid_cnv_new_split_GCN.npy\")\n",
    "gcn_test_gene = np.load(\"saved_output_data//Valid_gene_new_split_GCN.npy\")\n",
    "gcn_test_drugs = np.load(\"saved_output_data//Valid_drug_new_split_GCN.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb49425d-fc68-4f4c-a2f2-ab887f339465",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdr_test_gene.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67adef1c-087e-483c-8c3c-70b72a5565e0",
   "metadata": {},
   "source": [
    "Need to only use the target domain data - that is embeddings from test set only. Chose the first 10K as train and the rest as test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea94a9d6-111a-4233-b31e-5dea1c5ef245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDR\n",
    "# mutation\n",
    "cdr_train_mut = cdr_test_mut[:13816,:]\n",
    "# cdr_valid_mut = cdr_test_mut[10000:13816,:]\n",
    "cdr_test_mut = cdr_test_mut[13816:,:]\n",
    "print(cdr_train_mut.shape, cdr_test_mut.shape)\n",
    "\n",
    "# methylation\n",
    "cdr_train_methyl = cdr_test_methyl[:13816,:]\n",
    "# cdr_valid_methyl = cdr_test_methyl[10000:13816,:]\n",
    "cdr_test_methyl = cdr_test_methyl[13816:,:]\n",
    "print(cdr_train_methyl.shape,  cdr_test_methyl.shape)\n",
    "\n",
    "# expression\n",
    "cdr_train_gene = cdr_test_gene[:13816,:]\n",
    "# cdr_valid_gene = cdr_test_gene[10000:13816,:]\n",
    "cdr_test_gene = cdr_test_gene[13816:,:]\n",
    "print(cdr_train_gene.shape, cdr_test_gene.shape)\n",
    "\n",
    "# drugs\n",
    "cdr_train_drugs = cdr_test_drugs[:13816,:]\n",
    "# cdr_valid_drugs = cdr_test_drugs[10000:13816,:]\n",
    "cdr_test_drugs = cdr_test_drugs[13816:,:]\n",
    "print(cdr_train_drugs.shape, cdr_test_drugs.shape)\n",
    "\n",
    "# GCN\n",
    "# cnv\n",
    "gcn_train_cnv = gcn_test_cnv[:13816,:]\n",
    "# gcn_valid_cnv = gcn_test_cnv[10000:13816,:]\n",
    "gcn_test_cnv = gcn_test_cnv[13816:,:]\n",
    "print(gcn_train_cnv.shape, gcn_test_cnv.shape)\n",
    "\n",
    "# expression\n",
    "gcn_train_gene = gcn_test_gene[:13816,:]\n",
    "# gcn_valid_gene = gcn_test_gene[10000:13816,:]\n",
    "gcn_test_gene = gcn_test_gene[13816:,:]\n",
    "print(gcn_train_gene.shape, gcn_test_gene.shape)\n",
    "\n",
    "# drugs\n",
    "gcn_train_drugs = gcn_test_drugs[:13816,:]\n",
    "# gcn_valid_drugs = gcn_test_drugs[10000:13816,:]\n",
    "gcn_test_drugs = gcn_test_drugs[13816:,:]\n",
    "print(gcn_train_drugs.shape, gcn_test_drugs.shape)\n",
    "\n",
    "# response\n",
    "y_train = test_y[:13816,:]\n",
    "# y_valid = test_y[10000:13816,:]\n",
    "y_test = test_y[13816:,:]\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1246a77-8c6e-4dce-967b-347bb328bf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to aquire the needed omics, and then fit a PCA - let's not collapse any column in the X_t matrix\n",
    "\n",
    "def extract_PCA(train_cdr_omics, train_gcn_omics, test_cdr_omics, test_gcn_omics, train_cdr_drug, test_cdr_drug, train_gcn_drug, test_gcn_drug, y_train, y_test, keep_dims = 100):\n",
    "    # for CDR omics - PCA\n",
    "    if len(train_cdr_omics) == 1:\n",
    "        pca_cdr = PCA(n_components=train_cdr_omics[0].shape[1])\n",
    "        pca_cdr.fit(train_cdr_omics[0])\n",
    "        cdr_train_omics = pca_cdr.transform(train_cdr_omics[0])[:, :keep_dims]\n",
    "        # cdr_valid_omics = pca_cdr.transform(valid_cdr_omics[0])[:, :keep_dims]\n",
    "        cdr_test_omics = pca_cdr.transform(test_cdr_omics[0])[:, :keep_dims]\n",
    "    elif len(train_cdr_omics) == 2:\n",
    "        combined_omics_train = np.hstack(train_cdr_omics)\n",
    "        # combined_omics_valid = np.hstack(valid_cdr_omics)\n",
    "        combined_omics_test = np.hstack(test_cdr_omics)\n",
    "        pca_cdr = PCA(n_components=combined_omics_train.shape[1])\n",
    "        pca_cdr.fit(combined_omics_train)\n",
    "        cdr_train_omics = pca_cdr.transform(combined_omics_train)[:, :keep_dims]\n",
    "        # cdr_valid_omics = pca_cdr.transform(combined_omics_valid)[:, :keep_dims]\n",
    "        cdr_test_omics = pca_cdr.transform(combined_omics_test)[:, :keep_dims]\n",
    "    else:\n",
    "        combined_omics_train = np.hstack(train_cdr_omics)\n",
    "        # combined_omics_valid = np.hstack(valid_cdr_omics)\n",
    "        combined_omics_test = np.hstack(test_cdr_omics)\n",
    "        pca_cdr = PCA(n_components=combined_omics_train.shape[1])\n",
    "        pca_cdr.fit(combined_omics_train)\n",
    "        cdr_train_omics = pca_cdr.transform(combined_omics_train)[:, :keep_dims]\n",
    "        # cdr_valid_omics = pca_cdr.transform(combined_omics_valid)[:, :keep_dims]\n",
    "        cdr_test_omics = pca_cdr.transform(combined_omics_test)[:, :keep_dims]\n",
    "\n",
    "    # for GCN omics - PCA\n",
    "    if len(train_gcn_omics) == 1:\n",
    "        pca_gcn = PCA(n_components=train_gcn_omics[0].shape[1])\n",
    "        pca_gcn.fit(train_gcn_omics[0])\n",
    "        gcn_train_omics = pca_gcn.transform(train_gcn_omics[0])[:, :keep_dims]\n",
    "        # gcn_valid_omics = pca_gcn.transform(valid_gcn_omics[0])[:, :keep_dims]\n",
    "        gcn_test_omics = pca_gcn.transform(test_gcn_omics[0])[:, :keep_dims]\n",
    "    else:\n",
    "        combined_gcn_omics_train = np.hstack(train_gcn_omics)\n",
    "        # combined_gcn_omics_valid = np.hstack(valid_gcn_omics)\n",
    "        combined_gcn_omics_test = np.hstack(test_gcn_omics)\n",
    "        pca_gcn = PCA(n_components=combined_gcn_omics_train.shape[1])\n",
    "        pca_gcn.fit(combined_gcn_omics_train)\n",
    "        gcn_train_omics = pca_gcn.transform(combined_gcn_omics_train)[:, :keep_dims]\n",
    "        # gcn_valid_omics = pca_gcn.transform(combined_gcn_omics_valid)[:, :keep_dims]\n",
    "        gcn_test_omics = pca_gcn.transform(combined_gcn_omics_test)[:, :keep_dims]\n",
    "\n",
    "    # note that we are going to have the same drug embeddings, but the original EnKF has different embeddings from separately training the two networks - DualGCN and DeepCDR\n",
    "    # Let's include two methods for this for now, but need to discuss with Ved about this\n",
    "    \n",
    "    # cdr drugs\n",
    "    pca_cdr_drug = PCA(n_components=train_cdr_drug.shape[1])\n",
    "    pca_cdr_drug.fit(train_cdr_drug)\n",
    "    cdr_train_drugs = pca_cdr_drug.transform(train_cdr_drug)[:, :keep_dims]\n",
    "    # cdr_valid_drugs = pca_cdr_drug.transform(valid_cdr_drug)[:, :keep_dims]\n",
    "    cdr_test_drugs = pca_cdr_drug.transform(test_cdr_drug)[:, :keep_dims]\n",
    "\n",
    "\n",
    "    # gcn drugs\n",
    "    pca_gcn_drug = PCA(n_components=train_gcn_drug.shape[1])\n",
    "    pca_gcn_drug.fit(train_gcn_drug)\n",
    "    gcn_train_drugs = pca_gcn_drug.transform(train_gcn_drug)[:, :keep_dims]\n",
    "    # gcn_valid_drugs = pca_gcn_drug.transform(valid_gcn_drug)[:, :keep_dims]\n",
    "    gcn_test_drugs = pca_gcn_drug.transform(test_gcn_drug)[:, :keep_dims]\n",
    "\n",
    "    final_train = [cdr_train_drugs, cdr_train_omics, gcn_train_drugs, gcn_train_omics, y_train.astype(\"float32\")]\n",
    "    # final_valid = [cdr_valid_drugs, cdr_valid_omics, gcn_valid_drugs, gcn_valid_omics, y_valid.astype(\"float32\")]\n",
    "    final_test = [cdr_test_drugs, cdr_test_omics, gcn_test_drugs, gcn_test_omics, y_test.astype(\"float32\")]\n",
    "\n",
    "    return(final_train, final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335cd958-d3c3-4f0a-8346-a6b0e1206dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do this without methylation and cnv data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d6520f-cad4-4bd1-862a-0899f56b09bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cdr_omics = [cdr_train_gene, cdr_train_methyl, cdr_train_mut]\n",
    "train_gcn_omics = [gcn_train_cnv, gcn_train_gene]\n",
    "test_cdr_omics = [cdr_test_gene, cdr_test_methyl, cdr_test_mut]\n",
    "test_gcn_omics = [gcn_test_cnv, gcn_test_gene]\n",
    "train_cdr_drug = cdr_train_drugs\n",
    "test_cdr_drug = cdr_test_drugs\n",
    "train_gcn_drug = gcn_train_drugs\n",
    "test_gcn_drug = gcn_test_drugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952a10a6-6bb1-4fbb-9543-5054e58b859c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_list, test_list = extract_PCA(train_cdr_omics, train_gcn_omics, test_cdr_omics, test_gcn_omics, train_cdr_drug,  test_cdr_drug, train_gcn_drug,  test_gcn_drug, y_train, y_test, keep_dims = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b294d-c0d1-4da3-bfb8-5e4f49adc56b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56a7a59-e8c7-4b40-96de-af3142d4747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets_with_weights(batch_data, initial_ensembles, size_ens): \n",
    "    \n",
    "    target_dim = 1\n",
    "    \n",
    "    # weights_ann_1 = ann.get_weights()\n",
    "    \n",
    "    # h1  = ann.layers[1].output.shape[-1]\n",
    "\n",
    "    n_hidden_1 = len(weights_ann_1[0].ravel())\n",
    "    \n",
    "    hidden_weights_1 = initial_ensembles[:,:n_hidden_1].reshape( size_ens, batch_data.shape[1], h1)\n",
    "    \n",
    "    \n",
    "    hidden_output_1 = np.einsum('ij,kjl->kil', batch_data, hidden_weights_1)\n",
    "\n",
    "    \n",
    "    hidden_layer_bias_1 = initial_ensembles[:,n_hidden_1:(n_hidden_1 + h1)].reshape(size_ens, 1,  h1)\n",
    "\n",
    "\n",
    "    hidden_output_1 = hidden_output_1 + hidden_layer_bias_1\n",
    "\n",
    "    n_pred_weights_1 = len(weights_ann_1[2].ravel())\n",
    "\n",
    "    output_weights_1 = initial_ensembles[:,(n_hidden_1 + h1):(n_hidden_1 + h1 + n_pred_weights_1) ].reshape(size_ens, h1, target_dim)\n",
    "\n",
    "\n",
    "    output_1 = np.einsum('ijk,ikl->ijl', hidden_output_1, output_weights_1)\n",
    "\n",
    "\n",
    "    output_layer_bias_1 = initial_ensembles[:,(n_hidden_1 + h1 + n_pred_weights_1):(n_hidden_1 + h1 + n_pred_weights_1 + target_dim)].reshape(size_ens, 1, target_dim)\n",
    "\n",
    "\n",
    "    final_output_1 = output_1 + output_layer_bias_1\n",
    "    \n",
    "    final_output_1 = final_output_1[:,:, 0]\n",
    "    \n",
    "    # print(final_output_1.shape, initial_ensembles.shape)\n",
    "    \n",
    "    stack = np.hstack((final_output_1, initial_ensembles))\n",
    "\n",
    "    \n",
    "    return final_output_1, stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06f6361-ece8-40c7-ae02-f562b6f023fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ann(hidden = 32, input_shape = 256, output_shape = 1): \n",
    "    input_layer = tf.keras.layers.Input(shape = (input_shape))\n",
    "    hidden_layer = tf.keras.layers.Dense(hidden)\n",
    "    hidden_output = hidden_layer(input_layer)\n",
    "    pred_layer = tf.keras.layers.Dense(output_shape, activation = \"relu\")\n",
    "    pred_output = pred_layer(hidden_output)\n",
    "#     pred_output = tf.keras.layers.Activation(\"softmax\")(pred_output)\n",
    "    model = tf.keras.models.Model(input_layer, pred_output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2356abd-550a-4125-af88-ad8ff943ac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_initial_ensembles(num_weights, lambda1, size_ens):\n",
    "    mean_vec = np.zeros((num_weights,))\n",
    "    cov_matrix = lambda1*np.identity(num_weights)\n",
    "    mvn_samp = mvn(mean_vec, cov_matrix)\n",
    "    return mvn_samp.rvs(size_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4b76b4-080b-4330-9698-4527453b7a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_ann =  ann(hidden = 8, input_shape = 100, output_shape = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece0c9d9-6984-4c61-926e-dea295a190d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_ann_1 = samp_ann.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59640b1e-0b5d-4c28-a755-261b97d34da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1  = samp_ann.layers[1].output.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b151337a-f442-4798-845e-7a9973ead263",
   "metadata": {},
   "outputs": [],
   "source": [
    "h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb3c0f7-7e94-46be-bd93-8637bec59da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_ann.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6b5e55-7fa8-41cc-980a-f64d88abe88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_neurons = h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe60c6ae-fbbe-4a8f-aaa4-15558e919b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_ann_params = samp_ann.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed7850-b4d3-4ac6-b4c6-856b0fc90a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_train(catch_train, idxes): \n",
    "    # idxes = random.sample(range(0, catch_train[0].shape[0]), k = size)\n",
    "    # idxes = list(idxes)\n",
    "    data1, data2, data3, data4 = catch_train[0][idxes,:], catch_train[1][idxes,:], catch_train[2][idxes,:], catch_train[3][idxes,:]\n",
    "    \n",
    "    y_train = catch_train[-1][idxes].reshape(-1,1)\n",
    "    \n",
    "    return data1, data2, data3, data4, y_train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8face5-0b7d-40b4-a9d8-aa09a664bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_test( catch_test, size): \n",
    "    idxes = random.sample(range(0, catch_test[0].shape[0]), k = size)\n",
    "    idxes = list(idxes)\n",
    "    data1, data2, data3, data4 = catch_test[0][idxes,:], catch_test[1][idxes,:], catch_test[2][idxes,:], catch_test[3][idxes,:]\n",
    "    y_train = catch_test[-1][idxes].reshape(-1,1)\n",
    "    return data1, data2, data3, data4, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effad424-8e5e-4576-bb16-054599a16279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_valid( catch_valid, size): \n",
    "    idxes = random.sample(range(0, catch_valid[0].shape[0]), k = size)\n",
    "    idxes = list(idxes)\n",
    "    data1, data2, data3, data4 = catch_valid[0][idxes,:], catch_valid[1][idxes,:], catch_valid[2][idxes,:], catch_valid[3][idxes,:]\n",
    "    y_valid = catch_valid[-1][idxes].reshape(-1,1)\n",
    "    return data1, data2, data3, data4, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8527dd0e-82d6-45b6-941c-9ca4b80b50f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idxes = random.sample(range(0, train_list[0].shape[0]), k = train_list[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db35459-2da4-4833-b558-05c0471847f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_traina, data2_traina, data3_traina, data4_traina, y_traina =  prepare_data_train(train_list, train_idxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc2f059-0bdb-4a96-8304-1ffb80d7f22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1_test, data2_test, data3_test, data4_test, y_test =  prepare_data_test(test_list, size = test_list[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d78e0b-422f-41bb-8f21-cb488a804692",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_X_t(data1, data2, data3, data4, size_ens, var_weights = 1.0, var_weight_weights = 4.0, var_L = 1.0, var_D = 1.0):\n",
    "    # samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)\n",
    "    \n",
    "    initial_ensembles1 = generate_initial_ensembles(samp_ann_params, var_weights, size_ens)\n",
    "    data1_out1, data1_stack1 = get_targets_with_weights(data1, initial_ensembles1, size_ens = size_ens)\n",
    "    \n",
    "    initial_ensembles2 = generate_initial_ensembles(samp_ann_params, var_weights, size_ens)\n",
    "    data1_out2, data1_stack2 = get_targets_with_weights(data2, initial_ensembles2, size_ens = size_ens)\n",
    "    \n",
    "    initial_ensembles3 = generate_initial_ensembles(samp_ann_params, var_weights, size_ens)\n",
    "    data2_out1, data2_stack1 = get_targets_with_weights(data3, initial_ensembles3, size_ens = size_ens)\n",
    "    \n",
    "    initial_ensembles4 = generate_initial_ensembles(samp_ann_params, var_weights, size_ens)\n",
    "    data2_out2, data2_stack2 = get_targets_with_weights(data4, initial_ensembles4, size_ens = size_ens)   \n",
    "    \n",
    "    X_t = np.concatenate((np.expand_dims(data1_stack1, -1), np.expand_dims(data1_stack2, -1), \n",
    "                         np.expand_dims(data2_stack1, -1), np.expand_dims(data2_stack2, -1)), axis = -1)\n",
    "    \n",
    "    initial_ensembles_for_weights = generate_initial_ensembles(4, var_weight_weights, size_ens)\n",
    "    initial_ensembles_for_weights = np.expand_dims(initial_ensembles_for_weights,1)\n",
    "    \n",
    "    initial_ensembles_for_D1 = generate_initial_ensembles(1, var_D, size_ens).reshape(-1,1)\n",
    "    \n",
    "    initial_ensembles_for_D1_zero = np.zeros((size_ens,1,1)).reshape(-1,1)\n",
    "    initial_ensembles_for_D2_zero = np.zeros((size_ens,1,1)).reshape(-1,1)\n",
    "    \n",
    "    initial_ensembles_for_D3_zero = np.zeros((size_ens,1,1)).reshape(-1,1)\n",
    "    \n",
    "    initial_ensembles_for_D = np.concatenate((np.expand_dims(initial_ensembles_for_D1,1),\n",
    "                                                       np.expand_dims(initial_ensembles_for_D1_zero,1), \n",
    "                                                      np.expand_dims(initial_ensembles_for_D2_zero,1),\n",
    "                                                       np.expand_dims(initial_ensembles_for_D3_zero,1)), axis = 2)\n",
    "    \n",
    "    # print(X_t.shape, initial_ensembles_for_weights.shape)\n",
    "    \n",
    "    X_t = np.concatenate((X_t, initial_ensembles_for_weights, initial_ensembles_for_D), axis = 1)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles1, initial_ensembles2, initial_ensembles3, initial_ensembles4))\n",
    "    \n",
    "    return X_t, initial_ensembles, initial_ensembles_for_weights[:,0,:], initial_ensembles_for_D[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc80dc22-9d6c-4053-a2d9-b4bead0d9297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weighted_targets_with_weights(batch_data, initial_ensembles, size_ens, weights): \n",
    "    \n",
    "    target_dim = 1\n",
    "    \n",
    "\n",
    "    n_hidden_1 = len(weights_ann_1[0].ravel())\n",
    "    \n",
    "    hidden_weights_1 = initial_ensembles[:,:n_hidden_1].reshape( size_ens, batch_data.shape[1], h1)\n",
    "    \n",
    "    \n",
    "    hidden_output_1 = np.einsum('ij,kjl->kil', batch_data, hidden_weights_1)\n",
    "\n",
    "    \n",
    "    hidden_layer_bias_1 = initial_ensembles[:,n_hidden_1:(n_hidden_1 + h1)].reshape(size_ens, 1,  h1)\n",
    "\n",
    "\n",
    "    hidden_output_1 = hidden_output_1 + hidden_layer_bias_1\n",
    "\n",
    "    n_pred_weights_1 = len(weights_ann_1[2].ravel())\n",
    "\n",
    "    output_weights_1 = initial_ensembles[:,(n_hidden_1 + h1):(n_hidden_1 + h1 + n_pred_weights_1) ].reshape(size_ens, h1, target_dim)\n",
    "\n",
    "\n",
    "    output_1 = np.einsum('ijk,ikl->ijl', hidden_output_1, output_weights_1)\n",
    "\n",
    "\n",
    "    output_layer_bias_1 = initial_ensembles[:,(n_hidden_1 + h1 + n_pred_weights_1):(n_hidden_1 + h1 + n_pred_weights_1 + target_dim)].reshape(size_ens, 1, target_dim)\n",
    "\n",
    "\n",
    "    final_output_1 = output_1 + output_layer_bias_1\n",
    "    \n",
    "    final_output_1 = final_output_1[:,:, 0]\n",
    "    \n",
    "    final_output_1 = final_output_1*weights\n",
    "    \n",
    "    # print(final_output_1.shape, initial_ensembles.shape)\n",
    "    \n",
    "    stack = np.hstack((final_output_1, initial_ensembles))\n",
    "\n",
    "    \n",
    "    return final_output_1, stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c88cd0-66fa-4016-96c8-c36b23a6cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee845a6-8064-4d6b-b152-75e19f23e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fudging_beta = beta(1,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4351fa-cc0d-4322-9b31-372749f6c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_operation(data1, data2, data3, data4, combined_ensembles , size_ens, fudging_beta):\n",
    "    # samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)\n",
    "    params = samp_ann_params\n",
    "    initial_ensembles1 = combined_ensembles[:, :params]\n",
    "    initial_ensembles2 = combined_ensembles[:, params:(2*params)]\n",
    "    initial_ensembles3 = combined_ensembles[:, (2*params):(3*params)]\n",
    "    initial_ensembles4 = combined_ensembles[:, (3*params):(4*params)]\n",
    "\n",
    "    \n",
    "    initial_ensembles_for_weights = combined_ensembles[:, (4*params):(4*params + 4)]\n",
    "    \n",
    "    # initial_ensembles_for_L = combined_ensembles[:, (4*params + 4):(4*params + 4 + 4)]\n",
    "    \n",
    "    initial_ensembles_for_D = combined_ensembles[:,(4*params + 4):(4*params + 4 + 4 )]\n",
    "    \n",
    "    \n",
    "    softmax_weights = tf.math.softmax(initial_ensembles_for_weights).numpy()\n",
    "    \n",
    "    model_1 = softmax_weights[:, 0].reshape(-1,1) \n",
    "    \n",
    "    # model_1 = np.min(model_1 -fudging_factor)\n",
    "    \n",
    "    model_2 = softmax_weights[:, 1].reshape(-1,1) \n",
    "    \n",
    "    model_3 = softmax_weights[:, 2].reshape(-1,1) \n",
    "    \n",
    "    model_4 = softmax_weights[:, 3].reshape(-1,1)\n",
    "    \n",
    "    sum_weights = model_1 + model_2 + model_3 + model_4\n",
    "    \n",
    "    \n",
    "    # model_1_plus_model_2 = model_1 + model_2\n",
    "    \n",
    "    model_1 = model_1/sum_weights\n",
    "    \n",
    "    model_2 = model_2/sum_weights\n",
    "    \n",
    "    model_3 = model_3/sum_weights\n",
    "    \n",
    "    model_4 = model_4/sum_weights\n",
    "    \n",
    "    \n",
    "    # print(np.mean(model_1 + model_2))\n",
    "    \n",
    "    data1_out1, data1_stack1 = get_weighted_targets_with_weights(data1, initial_ensembles1, size_ens = size_ens,\n",
    "                                                                  weights=model_1)\n",
    "    \n",
    "    data1_out2, data1_stack2 = get_weighted_targets_with_weights(data2, initial_ensembles2, size_ens = size_ens,\n",
    "                                                                weights=model_2)\n",
    "    \n",
    "    data2_out1, data2_stack1 = get_weighted_targets_with_weights(data3, initial_ensembles3, size_ens = size_ens,\n",
    "                                                                 weights=model_3)\n",
    "    \n",
    "    data2_out2, data2_stack2 = get_weighted_targets_with_weights(data4, initial_ensembles4, size_ens = size_ens,\n",
    "                                                                  weights=model_4)   \n",
    "    \n",
    "    X_t = np.concatenate((np.expand_dims(data1_stack1, -1), np.expand_dims(data1_stack2, -1), \n",
    "                         np.expand_dims(data2_stack1, -1), np.expand_dims(data2_stack2, -1)), axis = -1)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles1, initial_ensembles2, initial_ensembles3, initial_ensembles4, \n",
    "                        initial_ensembles_for_weights, initial_ensembles_for_D))\n",
    "    \n",
    "    # print(X_t.shape)\n",
    "    \n",
    "    initial_ensembles_for_weights = np.expand_dims(initial_ensembles_for_weights,1)\n",
    "    \n",
    "    # initial_ensembles_for_L = np.expand_dims(initial_ensembles_for_L,1)\n",
    "    \n",
    "    initial_ensembles_for_D = np.expand_dims(initial_ensembles_for_D,1)\n",
    "    \n",
    "    # print(initial_ensembles_for_weights.shape)\n",
    "    \n",
    "    X_t = np.concatenate((X_t, initial_ensembles_for_weights, initial_ensembles_for_D), axis = 1)\n",
    "    \n",
    "    final_output = data1_out1 + data1_out2 + data2_out1 + data2_out2\n",
    "    \n",
    "    # weighted_psa = data1_out2 + data2_out2\n",
    "    \n",
    "    return X_t, initial_ensembles,final_output, model_1, model_2, model_3, model_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86268a3f-c79f-4113-a37d-b9477677785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_operation_test(data1, data2, data3, data4, combined_ensembles , size_ens):\n",
    "    # samp_ann =  ann(hidden = hidden_neurons, input_shape = 32, output_shape = 1)\n",
    "    params = samp_ann_params\n",
    "    initial_ensembles1 = combined_ensembles[:, :params]\n",
    "    initial_ensembles2 = combined_ensembles[:, params:(2*params)]\n",
    "    initial_ensembles3 = combined_ensembles[:, (2*params):(3*params)]\n",
    "    initial_ensembles4 = combined_ensembles[:, (3*params):(4*params)]\n",
    "\n",
    "    \n",
    "    initial_ensembles_for_weights = combined_ensembles[:, (4*params):(4*params + 4)]\n",
    "    \n",
    "    initial_ensembles_for_D = combined_ensembles[:,(4*params + 4):(4*params + 4 + 4)]\n",
    "    \n",
    "    \n",
    "    softmax_weights = tf.math.softmax(initial_ensembles_for_weights).numpy()\n",
    "    \n",
    "    model_1 = softmax_weights[:, :1].reshape(-1,1) \n",
    "    \n",
    "    # model_1 = np.min(model_1 -fudging_factor)\n",
    "    \n",
    "    model_2 = softmax_weights[:, 1:2].reshape(-1,1) \n",
    "    \n",
    "    model_3 = softmax_weights[:, 2:3].reshape(-1,1)\n",
    "    \n",
    "    model_4 = softmax_weights[:, 3:4].reshape(-1,1)\n",
    "    \n",
    "    sum_weights = model_1 + model_2 + model_3 + model_4\n",
    "    \n",
    "    \n",
    "    # model_1_plus_model_2 = model_1 + model_2\n",
    "    \n",
    "    model_1 = model_1/sum_weights\n",
    "    \n",
    "    model_2 = model_2/sum_weights\n",
    "    \n",
    "    model_3 = model_3/sum_weights\n",
    "    \n",
    "    model_4 = model_4/sum_weights\n",
    "    \n",
    "    data1_out1, data1_stack1 = get_weighted_targets_with_weights(data1, initial_ensembles1, size_ens = size_ens,\n",
    "                                                                  weights=model_1)\n",
    "    \n",
    "    data1_out2, data1_stack2 = get_weighted_targets_with_weights(data2, initial_ensembles2, size_ens = size_ens,\n",
    "                                                                weights=model_2)\n",
    "    \n",
    "    data2_out1, data2_stack1 = get_weighted_targets_with_weights(data3, initial_ensembles3, size_ens = size_ens,\n",
    "                                                                 weights=model_3)\n",
    "    \n",
    "    data2_out2, data2_stack2 = get_weighted_targets_with_weights(data4, initial_ensembles4, size_ens = size_ens,\n",
    "                                                                  weights=model_4)   \n",
    "    \n",
    "    X_t = np.concatenate((np.expand_dims(data1_stack1, -1), np.expand_dims(data1_stack2, -1), \n",
    "                         np.expand_dims(data2_stack1, -1), np.expand_dims(data2_stack2, -1)), axis = -1)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles1, initial_ensembles2, initial_ensembles3, initial_ensembles4, \n",
    "                        initial_ensembles_for_weights, initial_ensembles_for_D))\n",
    "    \n",
    "    # print(X_t.shape)\n",
    "    \n",
    "    initial_ensembles_for_weights = np.expand_dims(initial_ensembles_for_weights,1)\n",
    "    \n",
    "    # initial_ensembles_for_L = np.expand_dims(initial_ensembles_for_L,1)\n",
    "    \n",
    "    initial_ensembles_for_D = np.expand_dims(initial_ensembles_for_D,1)\n",
    "    \n",
    "    # print(initial_ensembles_for_weights.shape)\n",
    "    \n",
    "    X_t = np.concatenate((X_t, initial_ensembles_for_weights, initial_ensembles_for_D), axis = 1)\n",
    "    \n",
    "    final_output = data1_out1 + data1_out2 + data2_out1 + data2_out2\n",
    "    \n",
    "    return X_t, initial_ensembles, final_output, model_1, model_2, model_3, model_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f0d887-4ffa-4835-975b-b934518dcf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_weights = 4*(samp_ann.count_params() + 1 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aae74a-10f9-4eb2-af96-aa05ca0b20ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduction = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5d4efa-492f-48f2-91ed-7ccb0737709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_ens = total_weights//reduction\n",
    "# total_weights//reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b30af87-02ce-40dc-a0cf-c798f593e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "size_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a13254-46d6-48af-b55a-40e00f2f0461",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_t = [[1, 1, 1, 1]]\n",
    "G_t = np.array(G_t).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b752952-64f8-42b1-a8ba-df450facf0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf4bb8-856e-4823-a436-f00728c77ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(data1, data2, data3, data4, initial_ensembles, fudging_beta  =fudging_beta): \n",
    "    _,_, weighted_alogp, w1, w2, w3, w4 = forward_operation(data1, data2, data3, data4, initial_ensembles, size_ens = size_ens, fudging_beta = fudging_beta)\n",
    "    return weighted_alogp, w1, w2, w3, w4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7736e80f-9c3f-4d20-8ac1-2b143692b3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_test(data1, data2, data3, data4, initial_ensembles): \n",
    "    _,_, weighted_alogp, w1, w2, w3, w4 = forward_operation_test(data1, data2, data3, data4, initial_ensembles, size_ens = size_ens)\n",
    "    return weighted_alogp, w1, w2, w3, w4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1496ca-99a3-4e44-ac35-3f7e760d1c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mu_bar_G_bar(data1, data2, data3, data4, initial_ensembles, fudging_beta):\n",
    "    H_t = np.hstack((np.identity(data1.shape[0]), np.zeros((data1.shape[0], samp_ann_params + 1  + 1))))\n",
    "    mu_bar = initial_ensembles.mean(0)\n",
    "    X_t, _,_, _, _, _, _ = forward_operation(data1, data2, data3, data4, initial_ensembles, size_ens = size_ens, fudging_beta = fudging_beta)\n",
    "    X_t = X_t.transpose((0,2,1))\n",
    "    X_t = X_t.reshape(X_t.shape[0], X_t.shape[1]*X_t.shape[2])\n",
    "    script_H_t = np.kron(G_t.T, H_t)\n",
    "    G_u = (script_H_t@X_t.T)\n",
    "    G_u = G_u.T\n",
    "    G_bar = (G_u.mean(0)).ravel()\n",
    "    return mu_bar.reshape(-1,1), G_bar.reshape(-1,1), G_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd3bba-f580-42da-90dc-9b202d569dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_C_u(initial_ensembles, mu_bar, G_bar, G_u): \n",
    "    u_j_minus_u_bar = initial_ensembles - mu_bar.reshape(1,-1)\n",
    "    G_u_minus_G_bar = G_u -  G_bar.reshape(1,-1)\n",
    "    c = np.zeros((total_weights, G_bar.shape[0]))\n",
    "    for i in range(0, size_ens): \n",
    "        c += np.kron(u_j_minus_u_bar[i, :].T.reshape(-1,1), G_u_minus_G_bar[i,:].reshape(-1,1).T)\n",
    "    return c/size_ens, G_u_minus_G_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a515de7-b132-4321-9c23-3f815a60bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_D_u( G_bar, G_u): \n",
    "    G_u_minus_G_bar = G_u -  G_bar.reshape(1,-1)\n",
    "    d = np.zeros((G_bar.shape[0], G_bar.shape[0]))\n",
    "    for i in range(0, size_ens): \n",
    "        d += np.kron(G_u_minus_G_bar[i,:].T.reshape(-1,1), G_u_minus_G_bar[i,:].reshape(-1,1).T)\n",
    "    return d/size_ens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92499fd5-fba1-47e3-84bc-110ac10458e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e6f401-00ee-492f-9040-4683e5b13cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_D = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d93ef6f-e67c-4c72-9dd5-b67bbfbeb9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cov(shape, initial_ensembles):\n",
    "    cov_part = initial_ensembles[:, -4:-3]\n",
    "    cov_part = cov_part.mean(0)\n",
    "    variances1 = tf.math.softplus(cov_part).numpy()\n",
    "    n = shape\n",
    "    return variances1, np.identity(n)*variances1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93be2ed-5253-487f-9a08-e27e8b0add4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_updated_ensemble(data1, data2, data3, data4, initial_ensembles, y_train, size_ens = size_ens, inflation_factor = 1.0, fudging_beta = fudging_beta, \n",
    "                        fudging_var = None):\n",
    "    mu_bar, G_bar, G_u = calculate_mu_bar_G_bar(data1, data2, data3, data4, initial_ensembles, fudging_beta)\n",
    "    C, G_u_minus_G_bar = calculate_C_u(initial_ensembles, mu_bar, G_bar, G_u)\n",
    "    D = calculate_D_u( G_bar, G_u)\n",
    "    _, R_t = create_cov(data1.shape[0],initial_ensembles)\n",
    "    inflation = np.identity(R_t.shape[0])*inflation_factor\n",
    "    D_plus_cov = D + (R_t *inflation_factor)\n",
    "    D_plus_cov_inv = np.linalg.inv(D_plus_cov)\n",
    "    mid_quant = C@D_plus_cov_inv\n",
    "    noise_vec_mean = np.zeros((R_t.shape[0], ))\n",
    "    noise_mvn = mvn(noise_vec_mean, R_t)\n",
    "    fudging = noise_mvn.rvs(size_ens)\n",
    "    interim = (y_train.T.flatten().reshape(1,-1) + fudging)\n",
    "    right_quant = interim - G_u\n",
    "    mid_times_right = mid_quant@right_quant.T\n",
    "    updated_ensemble = (initial_ensembles + mid_times_right.T)\n",
    "    if fudging_var is not None: \n",
    "        mean_vec = np.zeros((updated_ensemble.shape[1],))\n",
    "        cov_mat = np.identity(updated_ensemble.shape[1])*fudging_var\n",
    "        fudging_for_updated_ensembles = mvn(mean_vec, cov_mat)\n",
    "        fudging_for_updated_ensembles_vec = fudging_for_updated_ensembles.rvs(size_ens)\n",
    "        updated_ensemble = updated_ensemble + fudging_for_updated_ensembles_vec\n",
    "    return updated_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6e972d-a753-4573-9e0b-d0ff39153cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4e9bef-2ccd-4eea-9605-6a2d093b11e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddca8b2-f652-400b-aca6-cd69c3d905f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955eb992-d51a-47b5-b415-cd5985fe5635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaef778-bcbb-45dc-86b0-8e257d16e410",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f1c4b3-9ce0-4d9f-99fc-727fd601d2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1591a6b2-58ba-4f67-a7a1-e5590bea5226",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b9423-6645-4f9b-9d58-12c7269960dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(idx, var_weights = 1.0, var_weight_weights = 1.0, var_D = 0.01, inflation_factor = 1.6, fudging_beta = beta(1,19), \n",
    "               fudging_var = 1e-3, print_true = True):\n",
    "    \n",
    "    # smiles_feats_train, rdkit_feats_train, smiles_feats_valid, rdkit_feats_valid, y_train, y_train_actual, y_valid, y_valid_actual, initial_ensembles  = prepare_data(idx, var_weights = var_weights, var_weight_weights =var_weight_weights, var_L = var_L, var_D = var_D)\n",
    "    \n",
    "\n",
    "    train_rmse = []\n",
    "    test_rmse = []\n",
    "    # print(\"done initializing\")\n",
    "    \n",
    "    w1_catch = []\n",
    "    w2_catch = []\n",
    "    w3_catch = []\n",
    "    w4_catch = []\n",
    "    \n",
    "    \n",
    "    train_idxes = random.sample(range(0, train_list[0].shape[0]), k = train_list[0].shape[0])\n",
    "    \n",
    "    train_chunks = list(chunks(train_idxes, batch_size))\n",
    "    \n",
    "    best_rmse_train = 10000\n",
    "    \n",
    "    data1_train, data2_train, data3_train, data4_train, y_train = prepare_data_train(train_list, train_chunks[0])\n",
    "    \n",
    "    _, initial_ensembles, initial_ensembles_for_weights, initial_ensembles_for_D = get_initial_X_t(data1_train, data2_train, data3_train, data4_train,\n",
    "                                                                                                 size_ens = size_ens, var_weights = var_weights,\n",
    "                                                                                                var_weight_weights = var_weight_weights,\n",
    "                                                                                                             var_D = var_D)\n",
    "    \n",
    "    initial_ensembles = np.hstack((initial_ensembles, initial_ensembles_for_weights, initial_ensembles_for_D))\n",
    "    patience = 0\n",
    "    \n",
    "    for i in range(0,300):\n",
    "        \n",
    "        train_chunks = random.sample(train_chunks, len(train_chunks))\n",
    "        \n",
    "        if print_true == True:\n",
    "            print(\"epoch number is \" +str(i))\n",
    "        \n",
    "        for chunk in (train_chunks):\n",
    "        \n",
    "            data1_train, data2_train, data3_train, data4_train, y_train = prepare_data_train(train_list, chunk)\n",
    "\n",
    "            initial_ensembles = get_updated_ensemble(data1_train, data2_train, data3_train, data4_train, initial_ensembles, y_train, size_ens = size_ens,\n",
    "                                                 inflation_factor = inflation_factor, fudging_beta = fudging_beta, fudging_var = fudging_var)\n",
    "        \n",
    "            G_u_train, w1, w2, w3, w4 = get_predictions(data1_traina, data2_traina, data3_traina, data4_traina, initial_ensembles, fudging_beta)\n",
    "    \n",
    "            li_train = np.percentile(G_u_train, axis = 0, q = (2.5, 97.5))[0,:].reshape(-1,1)    \n",
    "            ui_train = np.percentile(G_u_train, axis = 0, q = (2.5, 97.5))[1,:].reshape(-1,1)  \n",
    "    \n",
    "            width_train = ui_train - li_train\n",
    "            avg_width_train = width_train.mean(0)[0]\n",
    "    \n",
    "            ind_train = (y_traina >= li_train) & (y_traina <= ui_train)\n",
    "            coverage_train= ind_train.mean(0)[0]\n",
    "    \n",
    "            averaged_targets_train = G_u_train.mean(0).reshape(-1,1)\n",
    "            rmse_train = np.sqrt(((y_traina -averaged_targets_train)**2).mean(0))[0]\n",
    "        \n",
    "            pearsonr_train = pearsonr(averaged_targets_train.reshape(averaged_targets_train.shape[0],), \n",
    "                                 y_traina.reshape(y_traina.shape[0],))\n",
    "        \n",
    "            r_train = pearsonr_train[0]\n",
    "    \n",
    "            G_u_test, _, _, _, _ = get_predictions_test(data1_test, data2_test, data3_test, data4_test, initial_ensembles)\n",
    "    \n",
    "\n",
    "    \n",
    "            li_test = np.percentile(G_u_test, axis = 0, q = (2.5, 97.5))[0,:].reshape(-1,1)     \n",
    "            ui_test = np.percentile(G_u_test, axis = 0, q = (2.5, 97.5))[1,:].reshape(-1,1)   \n",
    "    \n",
    "            width_test = ui_test - li_test\n",
    "            avg_width_test = width_test.mean(0)[0]\n",
    "    \n",
    "            ind_test = (y_test >= li_test) & (y_test <= ui_test)\n",
    "            coverage_test= ind_test.mean(0)[0]\n",
    "    \n",
    "            averaged_targets_test = G_u_test.mean(0).reshape(-1,1)\n",
    "            rmse_test = np.sqrt(((y_test -averaged_targets_test)**2).mean(0))[0]  \n",
    "        \n",
    "            pearsonr_test = pearsonr(averaged_targets_test.reshape(averaged_targets_test.shape[0],), \n",
    "                                 y_test.reshape(y_test.shape[0],))\n",
    "        \n",
    "            r_test = pearsonr_test[0]\n",
    "            \n",
    "            train_rmse.append(rmse_train)\n",
    "            \n",
    "            test_rmse.append(rmse_test)\n",
    "            \n",
    "            \n",
    "            if print_true == True:\n",
    "                print(\"Training Coverage, Widths, RMSE, and Pearson R\")\n",
    "                print(coverage_train, avg_width_train, rmse_train, r_train)\n",
    "                print(\"Testing Coverage, Widths, RMSE, and Pearson R\")\n",
    "                print(coverage_test, avg_width_test, rmse_test, r_test)\n",
    "                print(\"Weight 1 Stats\")\n",
    "                print(w1.mean(), w1.std())\n",
    "                print(\"Weight 2 Stats\")\n",
    "                print(w2.mean(), w2.std())\n",
    "                print(\"Weight 3 Stats\")\n",
    "                print(w3.mean(), w3.std())\n",
    "                print(\"Weight 4 Stats\")\n",
    "                print(w4.mean(), w4.std())\n",
    "            \n",
    "            w1_catch.append([w1.mean(), w1.std()])\n",
    "            w2_catch.append([w2.mean(), w2.std()])\n",
    "            w3_catch.append([w3.mean(), w3.std()])\n",
    "            w4_catch.append([w4.mean(), w4.std()])\n",
    "            \n",
    "\n",
    "            if (rmse_train < best_rmse_train): \n",
    "                best_pearsonr_train = r_train\n",
    "            # print(\"went here\")\n",
    "                best_train_width_mean = avg_width_train.mean()\n",
    "                best_train_width = avg_width_train\n",
    "                # best_smiles_weight = w1.mean()\n",
    "                best_coverage_train = coverage_train\n",
    "                best_rmse_train = rmse_train\n",
    "                best_pearson_r = r_test\n",
    "                best_test_width = avg_width_test\n",
    "\n",
    "                best_coverage_test = coverage_test    \n",
    "                best_rmse_test = rmse_test\n",
    "                patience = 0\n",
    "                best_ensembles = initial_ensembles\n",
    "                \n",
    "                best_w1_weight = w1.mean()\n",
    "                best_w2_weight = w2.mean()\n",
    "                best_w3_weight = w3.mean()\n",
    "                best_w4_weight = w4.mean()\n",
    "                \n",
    "                best_test_preds = averaged_targets_test\n",
    "                best_li = li_test\n",
    "                best_ui = ui_test\n",
    "                \n",
    "                best_residuals = (y_test -averaged_targets_test)\n",
    "            \n",
    "            else:\n",
    "                patience = patience + 1\n",
    "            \n",
    "            if print_true == True:\n",
    "                print(\"Patience is\")\n",
    "                print(patience)\n",
    "                print('\\n')\n",
    "        \n",
    "            if patience > threshold:\n",
    "                # print(\"train_coverage\" + str(best_coverage_train), flush = True)\n",
    "                print(\"test_coverage\" + str(best_coverage_test), flush = True)\n",
    "                # print(\"train_width\" + str(best_train_width.tolist()), flush = True)\n",
    "                print(\"test_width\" + str(best_test_width), flush = True)\n",
    "                # print(\"smiles_weight\" + str(best_smiles_weight), flush = True)\n",
    "                # print(\"rmse_train\" + str(best_rmse_train.tolist()), flush = True)\n",
    "                print(\"rmse_test\" + str(best_rmse_test), flush = True)\n",
    "                print(\"pearson_r_test\" + str(best_pearson_r), flush = True)\n",
    "                weights = [best_w1_weight, best_w2_weight, best_w3_weight, best_w4_weight]\n",
    "                print(\"weights\", flush = True)\n",
    "                print(weights, flush = True)\n",
    "                print('\\n', flush = True)\n",
    "                # print(\"smiles_weight_ci\" + str([best_li_smiles_weight, best_ui_smiles_weight]), flush = True)\n",
    "                test_preds = ui_test\n",
    "            \n",
    "                return [best_train_width, best_coverage_train, best_rmse_train, best_test_width, best_coverage_test, best_rmse_test, best_pearson_r, best_ensembles, train_rmse, test_rmse, w1_catch, w2_catch, w3_catch, w4_catch, best_w1_weight, best_w2_weight, best_w3_weight, best_w4_weight, best_test_preds, best_li, best_ui, best_residuals]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e4e08d-4a33-40f3-accf-dff4e7df9c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b241405-dc53-4e22-bdc9-8ace62eca32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "best_train_width, best_coverage_train, best_rmse_train, best_test_width, best_coverage_test, best_rmse_test, best_pearson_r,  \\\n",
    "best_ensembles, train_rmse, test_rmse,   w1_catch, w2_catch, w3_catch, w4_catch, best_w1_weight, best_w2_weight, best_w3_weight, best_w4_weight, best_test_preds, best_li, best_ui, best_residuals = get_results(0, var_weights = 1.0, var_weight_weights = 10.0, var_D = 1.0, inflation_factor =1, fudging_beta = beta(1,19), \n",
    "           fudging_var = 0.005, print_true = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26294fa8-6907-41a7-b1f4-dddae0d49f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatterplot plots predicted and actual log IC50 of the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e9227-bd87-4b8a-bf85-8399d0bbcd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the preds wit best ensemble weights\n",
    "preds_test, _, _, _, _ = get_predictions_test(data1_test, data2_test, data3_test, data4_test, best_ensembles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0b0085-2d4a-40c9-ac70-4858fc7b9d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_preds_test = preds_test.mean(0).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62f629e-a931-4271-99f1-43551be3a4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.scatter(y_test, averaged_preds_test, alpha = 0.1)\n",
    "plt.axline((0,0), slope = 1, c = \"black\")\n",
    "plt.xlabel(\"Ground Truth Values\", fontweight = \"bold\",fontsize = 15)\n",
    "plt.ylabel(\"MEnKF-ANN Predicted Values\", fontweight = \"bold\", fontsize = 15)\n",
    "plt.xticks(fontsize = 10, fontweight = \"bold\")\n",
    "plt.yticks(fontsize = 10, fontweight = \"bold\")\n",
    "# fig.savefig('MEnKF_DeepCDR_DualGCN_Scatterplot.pdf', bbox_inches='tight', format = \"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca9b751-25b3-4370-aa3e-3c120a7c8d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual hisogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c817641-37b8-45b3-9b60-ae0e4a8bb307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95835acb-6bd1-4835-91c0-c4dc79cc219b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, mean, amplitude, standard_deviation):\n",
    "    return amplitude * np.exp( - (x - mean)**2 / (2*standard_deviation ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b33a287-ce33-4ba0-95b3-752801c22962",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_heights, bin_borders, _ = plt.hist(best_residuals, bins='auto', label='histogram')\n",
    "bin_centers = bin_borders[:-1] + np.diff(bin_borders) / 2\n",
    "popt, _ = curve_fit(gaussian, bin_centers, bin_heights, p0=[1., 0., 1.])\n",
    "\n",
    "x_interval_for_fit = np.linspace(bin_borders[0], bin_borders[-1], 10000)\n",
    "plt.plot(x_interval_for_fit, gaussian(x_interval_for_fit, *popt), label='fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7da470-1456-47a0-9f1d-bace6b0b1d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of confidence intervals, with observed values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2840ae34-7eb0-47c4-9c4b-1fbc56f2ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "catch_data = np.hstack((y_test, best_li, best_ui))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d164d59-e363-4e27-87d1-1cadd863664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "catch_data_df = pd.DataFrame(catch_data, columns = [ 'true_val', 'lower_CI', 'upper_CI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819b630a-13ad-4bef-8b9d-f9310ec8042e",
   "metadata": {},
   "outputs": [],
   "source": [
    "catch_data_df = catch_data_df.sort_values('true_val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079a6ed7-bc3a-435f-8498-915de95045bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig= plt.figure()\n",
    "plt.figure(figsize = (20,10))\n",
    "# fig, axs = plt.subplots(4, 2)\n",
    "plt.scatter(range(0,catch_data_df.shape[0]), catch_data_df['true_val'],  c = \"blue\", label = \"IC50\")\n",
    "plt.plot(range(0,catch_data_df.shape[0]), catch_data_df['lower_CI'],   c = \"green\",\n",
    "            label = \"Lower Prediction Interval\")\n",
    "plt.plot(range(0,catch_data_df.shape[0]), catch_data_df['upper_CI'],   c = \"green\",\n",
    "            label = \"Upper Prediction Interval\")\n",
    "# plt.fill_between(range(0,combo_df1.shape[0]), combo_df1[\"LI_IC50_Pred\"].values, combo_df1[\"UI_IC50_Pred\"].values,\n",
    "#                  color = \"green\", label = \"Empirical 95 % Prediction Interval\",alpha=0.6)\n",
    "\n",
    "plt.xlabel(\"Sample Index\", fontsize = 15, fontweight = \"bold\")\n",
    "plt.ylabel(\"Log(IC50)\", fontsize = 15, fontweight = \"bold\")\n",
    "plt.xticks( fontsize = 10, fontweight = \"bold\")\n",
    "plt.yticks( fontsize = 10, fontweight = \"bold\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env1)",
   "language": "python",
   "name": "tf_env1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
